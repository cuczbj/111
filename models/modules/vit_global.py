import torch
import torch.nn as nn
import torch.nn.functional as F
import models.vision_transformer as vision_transformer


class ViTGlobalModel(nn.Module):
    """
    åªç”¨ViTå…¨å±€ç‰¹å¾çš„æ¨¡åž‹
    ä¸ä½¿ç”¨RoI poolingï¼Œåªç”¨[CLS] token
    """
    def __init__(self, predicate_dim, hidden_dim=512, backbone="vit_base", 
                 pretrain_ckp="", freeze_backbone=True, **kwargs):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.predicate_dim = predicate_dim
        
        # åŠ è½½ViT backbone
        self.backbone = vision_transformer.__dict__[backbone](pretrain_ckp=pretrain_ckp)
        backbone_dim = self.backbone.embed_dim  # 768 for ViT-Base
        
        # æ˜¯å¦å†»ç»“backbone
        if freeze_backbone:
            print("\n" + "="*70)
            print("ðŸ§Š Freezing ViT backbone in ViTGlobalModel...")
            print("="*70)
            for name, param in self.backbone.named_parameters():
                param.requires_grad = False
                print(f"  â„ï¸  Frozen: backbone.{name}")
            
            frozen_params = sum(p.numel() for p in self.backbone.parameters())
            print(f"âœ… Successfully frozen {frozen_params:,} parameters in ViT backbone")
            print("="*70 + "\n")
        
        # Bboxç‰¹å¾ç¼–ç å™¨ï¼ˆå°†4ç»´bboxæ˜ å°„åˆ°é«˜ç»´ï¼‰
        bbox_embed_dim = 128
        self.bbox_encoder = nn.Sequential(
            nn.Linear(4, bbox_embed_dim),
            nn.ReLU(),
            nn.LayerNorm(bbox_embed_dim),
            nn.Linear(bbox_embed_dim, bbox_embed_dim),
            nn.ReLU(),
            nn.LayerNorm(bbox_embed_dim)
        )
        
        # ç‰¹å¾èžåˆå±‚
        # è¾“å…¥: [CLS] token (768) + subject_bbox_emb (128) + object_bbox_emb (128)
        fusion_input_dim = backbone_dim + bbox_embed_dim * 2
        
        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(0.3),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(0.3)
        )
        
        # å…³ç³»åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, predicate_dim)
        )
    
    def forward(self, full_im, bbox_s, bbox_o, predicate):
        """
        Args:
            full_im: (B, 3, H, W)
            bbox_s: (B, 4) - subject bbox, normalized
            bbox_o: (B, 4) - object bbox, normalized
            predicate: (B,) - predicate index
        """
        # 1. æå–ViTå…¨å±€ç‰¹å¾ ([CLS] token)
        vit_features = self.backbone(full_im)  # (B, 197, 768) for ViT-Base 224x224
        cls_token = vit_features[:, 0, :]  # (B, 768) - åªå–[CLS] token
        
        # 2. ç¼–ç bboxä¿¡æ¯
        bbox_s_emb = self.bbox_encoder(bbox_s)  # (B, 128)
        bbox_o_emb = self.bbox_encoder(bbox_o)  # (B, 128)
        
        # 3. èžåˆæ‰€æœ‰ç‰¹å¾
        combined = torch.cat([cls_token, bbox_s_emb, bbox_o_emb], dim=1)  # (B, 1024)
        fused_feature = self.fusion(combined)  # (B, hidden_dim)
        
        # 4. åˆ†ç±»
        logits = self.classifier(fused_feature)  # (B, predicate_dim)
        
        # 5. é€‰æ‹©å¯¹åº”predicateçš„è¾“å‡º
        predi_onehot = F.one_hot(predicate, num_classes=self.predicate_dim).float()
        output = torch.sum(logits * predi_onehot, dim=1)  # (B,)
        
        return output


class ViTBboxAttentionModel(nn.Module):
    """
    ä½¿ç”¨bboxä½ç½®ä¿¡æ¯å¼•å¯¼æ³¨æ„åŠ›
    è½¯æ€§åœ°èšç„¦åˆ°ç›¸å…³åŒºåŸŸ
    """
    def __init__(self, predicate_dim, hidden_dim=512, backbone="vit_base",
                 pretrain_ckp="", freeze_backbone=True, **kwargs):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.predicate_dim = predicate_dim
        
        self.backbone = vision_transformer.__dict__[backbone](pretrain_ckp=pretrain_ckp)
        backbone_dim = self.backbone.embed_dim
        
        if freeze_backbone:
            print("\n" + "="*70)
            print("ðŸ§Š Freezing ViT backbone in ViTBboxAttentionModel...")
            print("="*70)
            for param in self.backbone.parameters():
                param.requires_grad = False
            frozen_params = sum(p.numel() for p in self.backbone.parameters())
            print(f"âœ… Successfully frozen {frozen_params:,} parameters")
            print("="*70 + "\n")
        
        # ViTçš„patchæ•°é‡ (14x14 for 224x224 input with patch_size=16)
        self.num_patches_per_side = 14
        self.num_patches = self.num_patches_per_side ** 2
        
        # ä½ç½®ç¼–ç ç”Ÿæˆå™¨
        self.position_encoder = nn.Sequential(
            nn.Linear(2, 64),  # (x, y) -> 64
            nn.ReLU(),
            nn.Linear(64, 1)   # 64 -> 1 (attention score)
        )
        
        # Bboxç¼–ç å™¨
        bbox_embed_dim = 128
        self.bbox_encoder = nn.Sequential(
            nn.Linear(4, bbox_embed_dim),
            nn.ReLU(),
            nn.LayerNorm(bbox_embed_dim)
        )
        
        # ç‰¹å¾èžåˆ
        # subject_feature + object_feature + bbox_features
        fusion_input_dim = backbone_dim * 2 + bbox_embed_dim * 2
        
        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(0.3)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, predicate_dim)
        )
    
    def get_patch_positions(self, device):
        """
        èŽ·å–æ¯ä¸ªpatchçš„ä¸­å¿ƒä½ç½®
        Returns: (num_patches, 2) - å½’ä¸€åŒ–çš„(x, y)åæ ‡
        """
        positions = []
        for i in range(self.num_patches_per_side):
            for j in range(self.num_patches_per_side):
                # patchä¸­å¿ƒä½ç½®ï¼ˆå½’ä¸€åŒ–åˆ°[0,1]ï¼‰
                x = (i + 0.5) / self.num_patches_per_side
                y = (j + 0.5) / self.num_patches_per_side
                positions.append([x, y])
        
        return torch.tensor(positions, dtype=torch.float32, device=device)
    
    def compute_bbox_attention(self, bbox, patch_positions):
        """
        è®¡ç®—bboxå¯¹æ¯ä¸ªpatchçš„æ³¨æ„åŠ›æƒé‡
        Args:
            bbox: (B, 4) - (x1, x2, y1, y2)
            patch_positions: (num_patches, 2) - (x, y)
        Returns:
            attention_weights: (B, num_patches)
        """
        B = bbox.shape[0]
        num_patches = patch_positions.shape[0]
        
        # æ‰©å±•ç»´åº¦
        bbox = bbox.unsqueeze(1).expand(B, num_patches, 4)  # (B, 196, 4)
        patch_pos = patch_positions.unsqueeze(0).expand(B, num_patches, 2)  # (B, 196, 2)
        
        # è®¡ç®—æ¯ä¸ªpatchæ˜¯å¦åœ¨bboxå†…
        x, y = patch_pos[:, :, 0], patch_pos[:, :, 1]
        x1, x2, y1, y2 = bbox[:, :, 0], bbox[:, :, 1], bbox[:, :, 2], bbox[:, :, 3]
        
        # åœ¨bboxå†…çš„patchå¾—åˆ°æ›´é«˜æƒé‡
        inside = ((x >= x1) & (x <= x2) & (y >= y1) & (y <= y2)).float()
        
        # è®¡ç®—è·ç¦»bboxä¸­å¿ƒçš„è·ç¦»
        center_x = (x1 + x2) / 2
        center_y = (y1 + y2) / 2
        dist = torch.sqrt((x - center_x)**2 + (y - center_y)**2)
        
        # ä½ç½®ç¼–ç ç”Ÿæˆattention
        pos_encoding = self.position_encoder(patch_pos)  # (B, 196, 1)
        pos_encoding = pos_encoding.squeeze(-1)  # (B, 196)
        
        # ç»„åˆï¼šåœ¨bboxå†… + è·ç¦» + ä½ç½®ç¼–ç 
        attention_logits = inside * 2.0 - dist + pos_encoding
        attention_weights = F.softmax(attention_logits, dim=1)  # (B, 196)
        
        return attention_weights
    
    def forward(self, full_im, bbox_s, bbox_o, predicate):
        # 1. æå–ViTç‰¹å¾
        vit_features = self.backbone(full_im)  # (B, 197, 768)
        patch_features = vit_features[:, 1:, :]  # (B, 196, 768) - åŽ»æŽ‰[CLS]
        
        # 2. èŽ·å–patchä½ç½®
        patch_positions = self.get_patch_positions(full_im.device)  # (196, 2)
        
        # 3. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_s = self.compute_bbox_attention(bbox_s, patch_positions)  # (B, 196)
        attn_o = self.compute_bbox_attention(bbox_o, patch_positions)  # (B, 196)
        
        # 4. åŠ æƒèšåˆç‰¹å¾
        subject_feature = torch.sum(
            patch_features * attn_s.unsqueeze(-1),  # (B, 196, 768)
            dim=1
        )  # (B, 768)
        
        object_feature = torch.sum(
            patch_features * attn_o.unsqueeze(-1),
            dim=1
        )  # (B, 768)
        
        # 5. Bboxç¼–ç 
        bbox_s_emb = self.bbox_encoder(bbox_s)
        bbox_o_emb = self.bbox_encoder(bbox_o)
        
        # 6. èžåˆ
        combined = torch.cat([
            subject_feature, object_feature,
            bbox_s_emb, bbox_o_emb
        ], dim=1)
        
        fused_feature = self.fusion(combined)
        
        # 7. åˆ†ç±»
        logits = self.classifier(fused_feature)
        predi_onehot = F.one_hot(predicate, num_classes=self.predicate_dim).float()
        output = torch.sum(logits * predi_onehot, dim=1)
        
        return output
